{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Imports ###\n",
    "\n",
    "# general\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# preprocessing\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "# models\n",
    "from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor\n",
    "from xgboost import XGBRegressor\n",
    "from lightgbm import LGBMRegressor\n",
    "from sklearn.linear_model import Lasso, Ridge, ElasticNet\n",
    "\n",
    "# training and evaluation\n",
    "from sklearn.model_selection import KFold, GridSearchCV, RandomizedSearchCV, cross_val_score, cross_val_predict\n",
    "\n",
    "# analysis\n",
    "from sklearn.inspection import permutation_importance\n",
    "import shap\n",
    "\n",
    "# warnings and outputs\n",
    "import warnings\n",
    "from sklearn.exceptions import ConvergenceWarning\n",
    "\n",
    "warnings.filterwarnings(\"ignore\", category=UserWarning)\n",
    "warnings.filterwarnings(\"ignore\", category=ConvergenceWarning)\n",
    "warnings.filterwarnings('ignore', category=UserWarning, message=\".*ConvergenceWarning.*\")\n",
    "\n",
    "np.random.seed(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_data(\n",
    "    dataset, \n",
    "    label_column='Trabajo Final', \n",
    "    remove_columns=['Team'], \n",
    "    pca_components=None, \n",
    "    test_indices=None\n",
    "):\n",
    "    \"\"\"\n",
    "    Preprocess the data by separating features and target, scaling numerical data,\n",
    "    encoding categorical data, and optionally applying PCA. Optionally split into \n",
    "    train and test using provided test indices.\n",
    "\n",
    "    Parameters:\n",
    "    - dataset (pd.DataFrame): The input dataset.\n",
    "    - label_column (str): The target column.\n",
    "    - remove_columns (list): List of additional columns to remove.\n",
    "    - pca_components (int): Number of PCA components to retain (optional).\n",
    "    - test_indices (list or pd.Index, optional): Indices for the test set.\n",
    "\n",
    "    Returns:\n",
    "    - dict: A dictionary with processed train (and optionally test) data, targets.\n",
    "    \"\"\"\n",
    "    # Separate features and target\n",
    "    X = dataset.drop(columns=[label_column] + remove_columns)\n",
    "    y = dataset[label_column]\n",
    "\n",
    "    # Initialize train and test sets as None\n",
    "    X_train, X_test, y_train, y_test = X, None, y, None\n",
    "\n",
    "    # Split data into train and test sets if test_indices are provided\n",
    "    if test_indices is not None:\n",
    "        train_indices = dataset.index.difference(test_indices)\n",
    "        X_train, X_test = X.loc[train_indices], X.loc[test_indices]\n",
    "        y_train, y_test = y.loc[train_indices], y.loc[test_indices]\n",
    "\n",
    "    # Identify numerical and categorical columns\n",
    "    numerical_cols = X_train.select_dtypes(include=['number']).columns\n",
    "    categorical_cols = X_train.select_dtypes(include=['object', 'category']).columns\n",
    "\n",
    "    # Scale numerical columns\n",
    "    if not numerical_cols.empty:\n",
    "        scaler = StandardScaler()\n",
    "        X_train[numerical_cols] = scaler.fit_transform(X_train[numerical_cols])\n",
    "        if X_test is not None:\n",
    "            X_test[numerical_cols] = scaler.transform(X_test[numerical_cols])\n",
    "\n",
    "    # Encode categorical columns\n",
    "    if not categorical_cols.empty:\n",
    "        encoder = OneHotEncoder(sparse_output=False, handle_unknown='ignore')\n",
    "        encoded_train = pd.DataFrame(\n",
    "            encoder.fit_transform(X_train[categorical_cols]),\n",
    "            columns=encoder.get_feature_names_out(categorical_cols),\n",
    "            index=X_train.index\n",
    "        )\n",
    "        X_train = X_train.drop(columns=categorical_cols).join(encoded_train)\n",
    "\n",
    "        if X_test is not None:\n",
    "            encoded_test = pd.DataFrame(\n",
    "                encoder.transform(X_test[categorical_cols]),\n",
    "                columns=encoder.get_feature_names_out(categorical_cols),\n",
    "                index=X_test.index\n",
    "            )\n",
    "            X_test = X_test.drop(columns=categorical_cols).join(encoded_test)\n",
    "\n",
    "    # Apply PCA if specified\n",
    "    if pca_components:\n",
    "        pca = PCA(n_components=pca_components)\n",
    "        X_train_pca = pca.fit_transform(X_train)\n",
    "        X_train = pd.DataFrame(\n",
    "            X_train_pca, \n",
    "            columns=[f\"PC{i+1}\" for i in range(X_train_pca.shape[1])],\n",
    "            index=X_train.index\n",
    "        )\n",
    "        if X_test is not None:\n",
    "            X_test_pca = pca.transform(X_test)\n",
    "            X_test = pd.DataFrame(\n",
    "                X_test_pca, \n",
    "                columns=[f\"PC{i+1}\" for i in range(X_test_pca.shape[1])],\n",
    "                index=X_test.index\n",
    "            )\n",
    "\n",
    "    # Return processed train and (optionally) test sets\n",
    "    result = {\n",
    "        'X_train': X_train,\n",
    "        'y_train': y_train\n",
    "    }\n",
    "    \n",
    "    # If test set exists, return it too\n",
    "    if X_test is not None and y_test is not None:\n",
    "        result.update({\n",
    "            'X_test': X_test,\n",
    "            'y_test': y_test\n",
    "        })\n",
    "\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to evaluate models\n",
    "def evaluate_models(X, y, models, param_grids=None, random_configurations=None, random_seed=42):\n",
    "    print(f'Results for {len(X.columns)} features.')\n",
    "\n",
    "    # Initialize a dictionary to store results\n",
    "    results = {}\n",
    "    predictions = {}\n",
    "    best_model_name = None\n",
    "    best_model_score = -float('inf')\n",
    "\n",
    "    # Define cross-validation strategy\n",
    "    kfold = KFold(n_splits=5, shuffle=True, random_state=random_seed)\n",
    "\n",
    "    # Iterate over each model\n",
    "    for model_name, model in models.items():\n",
    "        print(f\"Evaluating {model_name}...\")\n",
    "\n",
    "        # Set random_state for models that support it\n",
    "        if hasattr(model, 'random_state'):\n",
    "            model.set_params(random_state=random_seed)\n",
    "        \n",
    "        # Get the parameter grid for the current model, if available\n",
    "        param_grid = param_grids.get(model_name, {}) if param_grids else {}\n",
    "        \n",
    "        # If there's no parameter grid, we just use the base model (no hyperparameter tuning)\n",
    "        if not param_grid:\n",
    "            print(f\"Using base model with default parameters for {model_name}.\")\n",
    "            # Use cross_val_score directly for base model evaluation without GridSearchCV\n",
    "            scores = cross_val_score(model, X, y, cv=kfold.split(X, y), scoring='neg_mean_squared_error', n_jobs=-1)\n",
    "            best_score = best_score\n",
    "            average_score = scores.mean()\n",
    "            best_params = None\n",
    "            fitted_model = model.fit(X, y)\n",
    "            grid_search = None\n",
    "            prediction = fitted_model.predict(X)\n",
    "        else:\n",
    "            # Perform Grid Search or Randomized Search with cross-validation if parameter grid exists\n",
    "            if random_configurations:\n",
    "                grid_search = RandomizedSearchCV(\n",
    "                    estimator=model, \n",
    "                    param_distributions=param_grid, \n",
    "                    n_iter=random_configurations,\n",
    "                    cv=kfold.split(X, y),\n",
    "                    n_jobs=-1,\n",
    "                    scoring='neg_mean_squared_error',\n",
    "                    random_state=random_seed,\n",
    "                    verbose=1)\n",
    "            else: \n",
    "                grid_search = GridSearchCV(estimator=model, \n",
    "                    param_grid=param_grid, \n",
    "                    cv=kfold.split(X, y),\n",
    "                    n_jobs=-1,\n",
    "                    scoring='neg_mean_squared_error',\n",
    "                    verbose=1)\n",
    "            \n",
    "            # Fit the model with the grid search\n",
    "            grid_search.fit(X, y)\n",
    "\n",
    "            # Extract results from GridSearchCV\n",
    "            mean_scores = grid_search.cv_results_['mean_test_score']\n",
    "            param_combinations = grid_search.cv_results_['params']\n",
    "\n",
    "            # Find the best parameter combination\n",
    "            best_index = mean_scores.argmax() # Index where the mean negative MSE is the least negative (highest)\n",
    "            best_params = param_combinations[best_index]\n",
    "            best_score = mean_scores[best_index]\n",
    "\n",
    "        print(f\"\\nBest Parameters: {best_params}\")\n",
    "        print(f\"Best Mean MSE (over 5 folds): {-best_score:.4f}\\n\")\n",
    "        \n",
    "        fitted_model = grid_search.best_estimator_\n",
    "        prediction = cross_val_predict(fitted_model, X, y, cv=kfold)\n",
    "\n",
    "        # Calculate R^2 score using cross-validated predictions\n",
    "        #r2_scores = cross_val_score(fitted_model, X, y, cv=kfold, scoring='r2', n_jobs=-1)\n",
    "        #mean_r2 = r2_scores.mean()\n",
    "        r2 = r2_score(y, prediction)\n",
    "        print(f\"Mean R^2 score (over 5 folds): {r2:.4f}\\n\")\n",
    "\n",
    "        # Save the best model and prediction\n",
    "        results[model_name] = {\n",
    "            'best_params': best_params,\n",
    "            'mse': best_score,\n",
    "            'r2': r2,\n",
    "            'fitted_model': fitted_model,\n",
    "            'grid_search': grid_search\n",
    "        }\n",
    "        predictions[model_name] = prediction\n",
    "        \n",
    "        # Track the best model\n",
    "        if best_score > best_model_score:\n",
    "            best_model_score = best_score\n",
    "            best_model_name = model_name\n",
    "\n",
    "    # Print the best model and its score\n",
    "    print(f\"The best model is {best_model_name} with a MSE score of {best_model_score:.4f}\")\n",
    "    \n",
    "    return results, predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define models and parameter grids\n",
    "models = {\n",
    "    'RandomForest': RandomForestRegressor(),\n",
    "    'GradientBoosting': GradientBoostingRegressor(),\n",
    "    'XGBoost': XGBRegressor(),\n",
    "    'Lasso': Lasso(),\n",
    "    'Ridge': Ridge(),\n",
    "    #'ElasticNet': ElasticNet(),\n",
    "    'LightGBM': LGBMRegressor()\n",
    "}\n",
    "\n",
    "param_grids = {\n",
    "    'RandomForest': {\n",
    "        'verbose': [0],\n",
    "        'n_estimators': [50, 100, 200, 500],\n",
    "        'max_depth': [3, 5, 7, 10],\n",
    "        'min_samples_split': [2, 5, 10],\n",
    "        'min_samples_leaf': [1, 2, 5],\n",
    "        'max_features': ['sqrt', 'log2'],\n",
    "        'bootstrap': [True, False]\n",
    "    },\n",
    "    'GradientBoosting': {\n",
    "        'verbose': [0],\n",
    "        'n_estimators': [50, 100, 200, 500],\n",
    "        'learning_rate': [0.01, 0.05, 0.1, 0.2],\n",
    "        'max_depth': [3, 5, 7, 10],\n",
    "        'min_samples_split': [2, 5, 10],\n",
    "        'min_samples_leaf': [1, 2, 5],\n",
    "        'subsample': [0.7, 0.8, 0.9, 1.0],\n",
    "        'max_features': ['sqrt', 'log2']\n",
    "    },\n",
    "    'XGBoost': {\n",
    "        'n_estimators': [50, 100, 200, 500],\n",
    "        'learning_rate': [0.01, 0.05, 0.1, 0.2],\n",
    "        'max_depth': [3, 5, 7, 10],\n",
    "        'min_child_weight': [1, 3, 5],\n",
    "        'subsample': [0.7, 0.8, 0.9, 1.0],\n",
    "        'colsample_bytree': [0.7, 0.8, 1.0],\n",
    "        'reg_alpha': [0, 0.01, 0.1, 1],\n",
    "        'reg_lambda': [0, 0.1, 0.5, 1],\n",
    "        'gamma': [0, 0.1, 0.5, 1]\n",
    "    },\n",
    "    'Lasso': {\n",
    "        'alpha': [0.001, 0.01, 0.1, 1, 10],\n",
    "        'max_iter': [1000, 5000, 10000],\n",
    "        'tol': [1e-3, 1e-4, 1e-5]\n",
    "    },\n",
    "    'Ridge': {\n",
    "        'alpha': [0.001, 0.01, 0.1, 1, 10, 100],\n",
    "        'max_iter': [1000, 5000, 10000],\n",
    "        'tol': [1e-3, 1e-4, 1e-5]\n",
    "    },\n",
    "    'ElasticNet': {\n",
    "        'alpha': [0.001, 0.01, 0.1, 1, 10],\n",
    "        'l1_ratio': [0.1, 0.5, 0.9],\n",
    "        'max_iter': [1000, 5000, 10000],\n",
    "        'tol': [1e-3, 1e-4, 1e-5]\n",
    "    },\n",
    "    'LightGBM': {\n",
    "        'verbose': [-1],\n",
    "        'n_estimators': [50, 100, 200, 500],\n",
    "        'learning_rate': [0.01, 0.05, 0.1],\n",
    "        'max_depth': [3, 5, 7, 10, -1],\n",
    "        'num_leaves': [31, 50, 100],\n",
    "        'min_child_samples': [5, 10, 20],\n",
    "        'subsample': [0.7, 0.8, 0.9, 1.0],\n",
    "        'colsample_bytree': [0.7, 0.8, 1.0],\n",
    "        'reg_alpha': [0, 0.01, 0.1, 1],\n",
    "        'reg_lambda': [0, 0.1, 0.5, 1]\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyze_results(model, X, y, X_test=None, y_test=None, model_name=\"Model\", max_features=20):\n",
    "    \"\"\"\n",
    "    Perform comprehensive evaluation of a model with optional test set support.\n",
    "\n",
    "    Parameters:\n",
    "    - model: The trained model to evaluate.\n",
    "    - X (pd.DataFrame): Training/validation features.\n",
    "    - y (pd.Series): Training/validation target.\n",
    "    - X_test (pd.DataFrame): Test features, optional.\n",
    "    - y_test (pd.Series): Test target, optional.\n",
    "    - model_name (str): Name of the model for display purposes.\n",
    "    - max_features (int): Number of features to display in SHAP and importance plots.\n",
    "\n",
    "    Returns:\n",
    "    None\n",
    "    \"\"\"\n",
    "    print(f\"=== Analysis for {model_name} ===\")\n",
    "    main_color = 'orange'\n",
    "\n",
    "    # Test Results (If Available)\n",
    "    if X_test is not None and y_test is not None:\n",
    "        print(\"\\nEvaluating on test data...\")\n",
    "\n",
    "        # Predict on test data\n",
    "        y_test_pred = model.predict(X_test)\n",
    "\n",
    "        # Residual Analysis: For test set\n",
    "        def plot_residuals_test(y_true, y_pred, baseline_residuals=None, title=None):\n",
    "            residuals = y_true - y_pred\n",
    "\n",
    "            # Baseline residuals (using mean prediction)\n",
    "            baseline_residuals = baseline_residuals if baseline_residuals is not None else y_true - np.mean(y)\n",
    "\n",
    "            plt.figure(figsize=(10, 6))\n",
    "            plt.scatter(y_pred, residuals, alpha=0.7, color=main_color, label='Residuals')\n",
    "            plt.scatter(y_pred, baseline_residuals, alpha=0.4, color='gray', label='Baseline Residuals', marker='x')\n",
    "            plt.axhline(0, color='red', linestyle='--', label='Zero Residual')\n",
    "            plt.xlabel(\"Predicted Values\")\n",
    "            plt.ylabel(\"Residuals\")\n",
    "            plt.title(title)\n",
    "            plt.legend()\n",
    "            plt.show()\n",
    "\n",
    "        print(\"\\nPlotting residuals for test data...\")\n",
    "        plot_residuals_test(y_test, y_test_pred, baseline_residuals=y_test - np.mean(y), title=f\"Residuals Plot for {model_name} (Test)\")\n",
    "\n",
    "        # Calculate R² score for test\n",
    "        r2_test = r2_score(y_test, y_test_pred)\n",
    "        print(f\"Test R² Score: {r2_test:.4f}\")\n",
    "\n",
    "        # Calculate MSE for test\n",
    "        mse_test = mean_squared_error(y_test, y_test_pred)\n",
    "        print(f\"Test MSE: {mse_test:.4f}\")\n",
    "\n",
    "        # Calculate MSE for baseline\n",
    "        mse_test_baseline = mean_squared_error(y_test, np.full_like(y_test, np.mean(y)))\n",
    "        print(f\"Baseline MSE (Mean Prediction): {mse_test_baseline:.4f}\")\n",
    "\n",
    "        # SHAP Analysis: On test data\n",
    "        print(\"\\nCalculating SHAP values for the test data...\")\n",
    "        explainer = shap.Explainer(model, X_test)\n",
    "        shap_values = explainer(X_test)\n",
    "        shap.summary_plot(shap_values, X_test, max_display=max_features)\n",
    "\n",
    "        # Permutation Feature Importance: On test data\n",
    "        print(\"\\nCalculating Permutation Feature Importance for the test data...\")\n",
    "        result = permutation_importance(model, X_test, y_test, n_repeats=100, random_state=42, n_jobs=-1)\n",
    "        importance_df = pd.DataFrame({\n",
    "            'feature': X_test.columns, \n",
    "            'importance': result.importances_mean\n",
    "        }).sort_values(by='importance', ascending=False)\n",
    "        print(importance_df)\n",
    "        plt.figure(figsize=(10, 6))\n",
    "        plt.barh(importance_df['feature'], importance_df['importance'], color=main_color)\n",
    "        plt.xlabel(\"Permutation Importance\")\n",
    "        plt.ylabel(\"Features\")\n",
    "        plt.title(f\"Permutation Importance for {model_name} (Test Data)\")\n",
    "        plt.gca().invert_yaxis()\n",
    "        plt.show()\n",
    "\n",
    "    # Training/Validation Results (Always Included)\n",
    "    print(\"\\nEvaluating on Training/validation data...\")\n",
    "\n",
    "    # Predict on Training/validation data\n",
    "    y_val_pred = model.predict(X)\n",
    "\n",
    "    # Residual Analysis: For Training/validation\n",
    "    def plot_residuals(y_true, y_pred, baseline_residuals=None, title=None):\n",
    "        residuals = y_true - y_pred\n",
    "\n",
    "        # Baseline residuals (using mean prediction)\n",
    "        baseline_residuals = baseline_residuals if baseline_residuals is not None else y_true - np.mean(y_true)\n",
    "\n",
    "        plt.figure(figsize=(10, 6))\n",
    "        plt.scatter(y_pred, residuals, alpha=0.7, color=main_color, label='Residuals')\n",
    "        plt.scatter(y_pred, baseline_residuals, alpha=0.4, color='gray', label='Baseline Residuals', marker='x')\n",
    "        plt.axhline(0, color='red', linestyle='--', label='Zero Residual')\n",
    "        plt.xlabel(\"Predicted Values\")\n",
    "        plt.ylabel(\"Residuals\")\n",
    "        plt.title(title)\n",
    "        plt.legend()\n",
    "        plt.show()\n",
    "\n",
    "    print(\"\\nPlotting residuals for Training/validation data...\")\n",
    "    plot_residuals(y, y_val_pred, baseline_residuals=y - np.mean(y), title=f\"Residuals Plot for {model_name} (Training/Validation)\")\n",
    "\n",
    "    # Calculate R² score for Training/validation\n",
    "    r2_val = r2_score(y, y_val_pred)\n",
    "    print(f\"Training/Validation R² Score: {r2_val:.4f}\")\n",
    "\n",
    "    # Calculate MSE for Training/validation\n",
    "    mse_val = mean_squared_error(y, y_val_pred)\n",
    "    print(f\"Training/Validation MSE: {mse_val:.4f}\")\n",
    "\n",
    "    # Calculate MSE for baseline\n",
    "    mse_val_baseline = mean_squared_error(y, np.full_like(y, np.mean(y)))\n",
    "    print(f\"Baseline MSE (Mean Prediction): {mse_val_baseline:.4f}\")\n",
    "\n",
    "    # SHAP Analysis: On Training/validation data\n",
    "    print(\"\\nCalculating SHAP values for the Training/validation data...\")\n",
    "    explainer = shap.Explainer(model, X)\n",
    "    shap_values = explainer(X)\n",
    "    shap.summary_plot(shap_values, X, max_display=max_features)\n",
    "\n",
    "    # Permutation Feature Importance: On Training/validation data\n",
    "    print(\"\\nCalculating Permutation Feature Importance for the Training/validation data...\")\n",
    "    result = permutation_importance(model, X, y, n_repeats=100, random_state=42, n_jobs=-1)\n",
    "    importance_df = pd.DataFrame({\n",
    "        'feature': X.columns, \n",
    "        'importance': result.importances_mean\n",
    "    }).sort_values(by='importance', ascending=False)\n",
    "    print(importance_df)\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.barh(importance_df['feature'], importance_df['importance'], color=main_color)\n",
    "    plt.xlabel(\"Permutation Importance\")\n",
    "    plt.ylabel(\"Features\")\n",
    "    plt.title(f\"Permutation Importance for {model_name} (Training/Validation Data)\")\n",
    "    plt.gca().invert_yaxis()\n",
    "    plt.show()\n",
    "\n",
    "    print(\"\\nAnalysis completed.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load data\n",
    "prediction_data = pd.read_excel('prediction_data.xlsx', sheet_name='Sheet1')\n",
    "\n",
    "# Determine test indices based on the first 2 characters of the \"Team\" column\n",
    "test_indices = prediction_data[prediction_data['Team'].str.startswith('24')].index\n",
    "\n",
    "# Use the preprocess_data function\n",
    "processed_data = preprocess_data(\n",
    "    dataset=prediction_data, \n",
    "    label_column='Trabajo Final',\n",
    "    remove_columns=['Team'],\n",
    "    pca_components = 7,\n",
    "    test_indices=None\n",
    ")\n",
    "\n",
    "# Access processed data\n",
    "X_train = processed_data['X_train']\n",
    "#X_test = processed_data['X_test']\n",
    "y_train = processed_data['y_train']\n",
    "#y_test = processed_data['y_test']\n",
    "\n",
    "# Evaluate all models\n",
    "results, predictions = evaluate_models(X_train, y_train, models, param_grids=param_grids, random_configurations=10000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "analyze_results(results['LightGBM']['fitted_model'], X_train, y_train, X_test=X_test, y_test=y_test, model_name='LightGBM', max_features=20)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "coin_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
